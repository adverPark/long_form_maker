import time
from datetime import date
from google.genai import types
from .base import BaseStepService
from apps.pipeline.models import Research


class ResearcherService(BaseStepService):
    """ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ - ëŒ€ë³¸ ê³„íšì˜ ë¦¬ì„œì¹˜ í•„ìš” í•­ëª©ì„ ì¡°ì‚¬

    ì›Œí¬í”Œë¡œ:
    1. YouTube ìˆ˜ì§‘ â†’ 2. ìë§‰ ë¶„ì„ â†’ 3. ëŒ“ê¸€ ë¶„ì„ â†’ 4. ëŒ€ë³¸ ê³„íš â†’ 5. ë¦¬ì„œì¹˜

    ëŒ€ë³¸ ê³„íšì—ì„œ ë„ì¶œëœ 'ë¦¬ì„œì¹˜ í•„ìš” í•­ëª©'ì„ í•˜ë‚˜ì”© ê²€ìƒ‰í•˜ì—¬ ì¡°ì‚¬í•©ë‹ˆë‹¤.
    """

    agent_name = 'researcher'

    DEFAULT_PROMPT = """ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ì½˜í…ì¸  ì œì‘ì„ ìœ„í•œ ë¦¬ì„œì¹˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ğŸš¨ í•µì‹¬ ì›ì¹™: ìµœì‹  ì •ë³´!

- **ì˜¤ëŠ˜ ë‚ ì§œ: {today}**
- ë°˜ë“œì‹œ **ê°€ì¥ ìµœì‹  ìë£Œ**ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”
- ê²€ìƒ‰í•  ë•Œ "{year}" ë˜ëŠ” "ìµœì‹ "ì„ í‚¤ì›Œë“œì— í¬í•¨í•˜ì„¸ìš”
- **ìˆ˜ì¹˜/í†µê³„ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ë°˜ë“œì‹œ ì—°ë„ë¥¼ ëª…ì‹œ**í•˜ì„¸ìš”
  - âŒ "íì—…ë¥ ì´ 30%ì— ë‹¬í•œë‹¤"
  - âœ… "2024ë…„ ê¸°ì¤€ íì—…ë¥ ì´ 30%ì— ë‹¬í•œë‹¤" ë˜ëŠ” "2025ë…„ 1ë¶„ê¸° íì—…ë¥ ì´..."
- ìµœì‹  ìë£Œë¥¼ êµ¬í•˜ì§€ ëª»í•œ ê²½ìš°, í•´ë‹¹ ìˆ˜ì¹˜ê°€ ëª‡ ë…„ë„ ìë£Œì¸ì§€ ë°˜ë“œì‹œ í‘œê¸°í•˜ì„¸ìš”

## ì¡°ì‚¬ ë°©ë²•
1. ë¦¬ì„œì¹˜ í•„ìš” í•­ëª©ì„ í™•ì¸í•©ë‹ˆë‹¤
2. ê° í•­ëª©ì— ëŒ€í•´ search_web ë„êµ¬ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤ (ìµœì‹  ìë£Œ ìš°ì„ !)
3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ì—¬ Markdown í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤

## ì¶œë ¥ í˜•ì‹ (Markdown)

# ë¦¬ì„œì¹˜ ê²°ê³¼

## 1. [ì²« ë²ˆì§¸ í•­ëª©]
- ì¡°ì‚¬ ë‚´ìš© (ìˆ˜ì¹˜ëŠ” ì—°ë„ ëª…ì‹œ!)
- ì¶œì²˜: [URL]

## 2. [ë‘ ë²ˆì§¸ í•­ëª©]
- ì¡°ì‚¬ ë‚´ìš©
- ì¶œì²˜: [URL]

(ì´í•˜ ë™ì¼)

---

ëª¨ë“  í•­ëª©ì„ ì¡°ì‚¬í•œ í›„ ê²°ê³¼ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”."""

    # ì¬ì‹œë„ ì„¤ì •
    MAX_RETRIES = 3
    RETRY_DELAY = 30  # ì´ˆ

    def __init__(self, execution):
        super().__init__(execution)
        self._search_count = 0
        self._all_sources = []

    def execute(self):
        self.update_progress(5, 'ë¦¬ì„œì¹˜ ì‹œì‘...')

        # ëŒ€ë³¸ ê³„íš í™•ì¸ (í•„ìˆ˜)
        if not hasattr(self.project, 'research') or not self.project.research:
            raise ValueError('ë¨¼ì € ëŒ€ë³¸ ê³„íšì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.')

        content_analysis = self.project.research.content_analysis or {}
        script_plan = content_analysis.get('script_plan', '')

        if not script_plan:
            raise ValueError('ëŒ€ë³¸ ê³„íšì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 5. ëŒ€ë³¸ ê³„íšì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.')

        # ì£¼ì œ ì •ë³´
        topic_title = ''
        if hasattr(self.project, 'topic') and self.project.topic:
            topic_title = self.project.topic.title

        self.log('ëŒ€ë³¸ ê³„íš ê¸°ë°˜ ë¦¬ì„œì¹˜ ì‹œì‘', 'info')

        # ì¤‘ê°„ ë°ì´í„° ë³µì›
        self._restore_intermediate_data()

        if self._search_count > 0:
            self.log(f'ì´ì „ ê²€ìƒ‰ {self._search_count}ê°œ ë³µì›ë¨', 'info')

        self.update_progress(10, 'ë¦¬ì„œì¹˜ í•„ìš” í•­ëª© ì¡°ì‚¬ ì¤‘...')

        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        result_text = self._run_agent(script_plan)

        # DBì— ì €ì¥ (Markdown í…ìŠ¤íŠ¸ë¡œ)
        self.update_progress(95, 'ê²°ê³¼ ì €ì¥ ì¤‘...')
        self._save_research(topic_title, result_text)

        # ì¤‘ê°„ ë°ì´í„° ì •ë¦¬
        self._clear_intermediate_data()

        self.log(f'ë¦¬ì„œì¹˜ ì™„ë£Œ (ê²€ìƒ‰ {self._search_count}íšŒ)', 'result')
        self.update_progress(100, f'ë¦¬ì„œì¹˜ ì™„ë£Œ (ê²€ìƒ‰ {self._search_count}íšŒ)')

    def _restore_intermediate_data(self):
        """ì¤‘ê°„ ì €ì¥ ë°ì´í„° ë³µì›"""
        data = self.execution.intermediate_data or {}

        if data.get('searches'):
            self._search_count = len(data['searches'])
            for search in data['searches']:
                self._all_sources.extend(search.get('sources', []))

    def _save_intermediate_data(self, query: str, text: str, sources: list):
        """ê²€ìƒ‰ ê²°ê³¼ ì¤‘ê°„ ì €ì¥"""
        data = self.execution.intermediate_data or {}

        if 'searches' not in data:
            data['searches'] = []

        data['searches'].append({
            'query': query,
            'summary': text,  # ì „ì²´ ìš”ì•½ ì €ì¥ (ì˜ë¦¬ì§€ ì•ŠìŒ)
            'sources': sources
        })

        self.execution.intermediate_data = data
        self.execution.save(update_fields=['intermediate_data'])

    def _clear_intermediate_data(self):
        """ì¤‘ê°„ ë°ì´í„° ì •ë¦¬"""
        self.execution.intermediate_data = {}
        self.execution.save(update_fields=['intermediate_data'])

    def _get_previous_context(self) -> str:
        """ì´ì „ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        data = self.execution.intermediate_data or {}
        searches = data.get('searches', [])

        if not searches:
            return ""

        context = "\n\n## ì´ì „ì— ê²€ìƒ‰í•œ ë‚´ìš©:\n"
        for i, search in enumerate(searches, 1):
            context += f"\n### ê²€ìƒ‰ {i}: {search['query']}\n"
            # ì „ì²´ ìš”ì•½ ì‚¬ìš© (summary í•„ë“œ)
            summary = search.get('summary', search.get('text', ''))
            context += summary[:1000] + ("..." if len(summary) > 1000 else "") + "\n"

        return context

    def _search_web_with_retry(self, query: str) -> str:
        """ì›¹ ê²€ìƒ‰ (ì¬ì‹œë„ í¬í•¨)"""
        last_error = None

        for attempt in range(self.MAX_RETRIES):
            try:
                return self._search_web(query, is_retry=(attempt > 0))
            except Exception as e:
                last_error = e
                if attempt < self.MAX_RETRIES - 1:
                    wait_time = self.RETRY_DELAY * (attempt + 1)
                    self.log(
                        f'ê²€ìƒ‰ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.MAX_RETRIES}): {str(e)}. '
                        f'{wait_time}ì´ˆ í›„ ì¬ì‹œë„...',
                        'error'
                    )
                    time.sleep(wait_time)

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        self.log(f'ê²€ìƒ‰ ìµœì¢… ì‹¤íŒ¨: {str(last_error)}', 'error')
        return f"ê²€ìƒ‰ ì‹¤íŒ¨ (3íšŒ ì¬ì‹œë„ í›„): {str(last_error)}"

    def _search_web(self, query: str, is_retry: bool = False) -> str:
        """ì›¹ ê²€ìƒ‰ ë„êµ¬ - Geminiê°€ í˜¸ì¶œí•¨

        Args:
            query: ê²€ìƒ‰í•  ë‚´ìš© (ì˜ˆ: "ìì˜ì—… íì—…ë¥  2025 í†µê³„")
            is_retry: ì¬ì‹œë„ì¸ ê²½ìš° True (ì¹´ìš´íŠ¸ ì¦ê°€ ì•ˆ í•¨)

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸
        """
        if not is_retry:
            self._search_count += 1
        self.log(f'ê²€ìƒ‰ #{self._search_count}: {query}', 'search')

        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (10~90% ë²”ìœ„)
        progress = min(10 + (self._search_count * 5), 90)
        self.update_progress(progress, f'ê²€ìƒ‰ ì¤‘: {query[:30]}...')

        # Google Search groundingìœ¼ë¡œ ê²€ìƒ‰
        result = self.call_gemini_with_search(
            f"ë‹¤ìŒì„ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”: {query}"
        )

        text = result.get('text') or ''
        sources = result.get('sources') or []

        # ì¶œì²˜ ì €ì¥
        self._all_sources.extend(sources)

        # ì¤‘ê°„ ì €ì¥
        self._save_intermediate_data(query, text, sources)

        self.log(f'ê²€ìƒ‰ ì™„ë£Œ: {len(sources)}ê°œ ì¶œì²˜', 'result', {
            'query': query,
            'sources_count': len(sources),
            'text_preview': text[:150] if text else ''
        })

        # ì¶œì²˜ ì •ë³´ í¬í•¨í•´ì„œ ë°˜í™˜
        source_info = ""
        if sources:
            source_info = "\n\nì¶œì²˜:\n" + "\n".join(
                f"- {s.get('title', 'N/A')}: {s.get('url', '')}"
                for s in sources[:5]
            )

        return text + source_info

    def _call_agent_with_retry(self, client, model_name, contents, config) -> any:
        """ì—ì´ì „íŠ¸ í˜¸ì¶œ (ì¬ì‹œë„ í¬í•¨)"""
        last_error = None

        for attempt in range(self.MAX_RETRIES):
            try:
                response = client.models.generate_content(
                    model=model_name,
                    contents=contents,
                    config=config
                )
                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
                self.track_usage(response, model_name)
                return response
            except Exception as e:
                last_error = e
                error_str = str(e).lower()

                # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì˜¤ë¥˜ì¸ì§€ í™•ì¸
                retriable = any(keyword in error_str for keyword in [
                    'overload', 'rate limit', 'quota', '429', '503', '500',
                    'timeout', 'unavailable', 'resource exhausted'
                ])

                if retriable and attempt < self.MAX_RETRIES - 1:
                    wait_time = self.RETRY_DELAY * (attempt + 1)
                    self.log(
                        f'API ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{self.MAX_RETRIES}): {str(e)[:100]}. '
                        f'{wait_time}ì´ˆ í›„ ì¬ì‹œë„...',
                        'error'
                    )
                    time.sleep(wait_time)
                else:
                    raise

        raise last_error

    def _run_agent(self, script_plan: str) -> str:
        """ì—ì´ì „íŠ¸ ë£¨í”„ ì‹¤í–‰ - ëŒ€ë³¸ ê³„íš ê¸°ë°˜ ë¦¬ì„œì¹˜

        Args:
            script_plan: ëŒ€ë³¸ ê³„íš (ë¦¬ì„œì¹˜ í•„ìš” í•­ëª© í¬í•¨)

        Returns:
            ë¦¬ì„œì¹˜ ê²°ê³¼ (Markdown í…ìŠ¤íŠ¸)
        """
        client = self.get_client()
        model_name = self.get_model_name()

        # ê²€ìƒ‰ ë„êµ¬ ì •ì˜
        search_tool_declaration = types.FunctionDeclaration(
            name="search_web",
            description="ì›¹ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ë¦¬ì„œì¹˜ í•„ìš” í•­ëª©ì„ ì¡°ì‚¬í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.",
            parameters={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "ê²€ìƒ‰í•  ë‚´ìš©"
                    }
                },
                "required": ["query"]
            }
        )

        # ì´ì „ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸
        previous_context = self._get_previous_context()

        user_message = f"""## ëŒ€ë³¸ ê³„íš

{script_plan}

---

ìœ„ ëŒ€ë³¸ ê³„íšì„ ë³´ê³ , **ë¦¬ì„œì¹˜ í•„ìš” í•­ëª©**ì„ ì´í•´í•œ í›„ í•˜ë‚˜ì”© ëª¨ë‘ ì¡°ì‚¬í•´ì£¼ì„¸ìš”.

search_web ë„êµ¬ë¡œ ê° í•­ëª©ì„ ê²€ìƒ‰í•˜ê³ , ì™„ë£Œë˜ë©´ Markdown í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.
{previous_context}"""

        contents = [
            types.Content(
                role="user",
                parts=[types.Part(text=user_message)]
            )
        ]

        # ì„¤ì • - ì˜¤ëŠ˜ ë‚ ì§œ ì£¼ì…
        today = date.today()
        system_prompt = self.DEFAULT_PROMPT.format(
            today=today.strftime('%Yë…„ %mì›” %dì¼'),
            year=today.year
        )
        config = types.GenerateContentConfig(
            system_instruction=system_prompt,
            tools=[types.Tool(function_declarations=[search_tool_declaration])]
        )

        # ì—ì´ì „íŠ¸ ë£¨í”„ (ìµœëŒ€ 20íšŒ)
        max_iterations = 20

        for i in range(max_iterations):
            self.log(f'ì—ì´ì „íŠ¸ í„´ {i+1}/{max_iterations}', 'info')

            try:
                response = self._call_agent_with_retry(
                    client, model_name, contents, config
                )
            except Exception as e:
                self.log(f'API ìµœì¢… ì‹¤íŒ¨: {str(e)}', 'error')
                return self._build_partial_result()

            # ì‘ë‹µ ì²˜ë¦¬
            if not response.candidates:
                self.log('ì‘ë‹µ ì—†ìŒ', 'error')
                break

            candidate = response.candidates[0]

            # contentê°€ ì—†ëŠ” ê²½ìš° ì²´í¬
            if not candidate.content or not candidate.content.parts:
                self.log(f'ì‘ë‹µì— contentê°€ ì—†ìŒ (finish_reason: {getattr(candidate, "finish_reason", "unknown")})', 'warning')
                continue

            # Function Call í™•ì¸
            function_calls = []
            text_response = ""

            for part in candidate.content.parts:
                if hasattr(part, 'function_call') and part.function_call:
                    function_calls.append(part.function_call)
                elif hasattr(part, 'text') and part.text:
                    text_response += part.text

            # Function Call ì²˜ë¦¬
            if function_calls:
                # ëª¨ë¸ ì‘ë‹µì„ contentsì— ì¶”ê°€
                contents.append(candidate.content)

                # ê° í•¨ìˆ˜ í˜¸ì¶œ ì‹¤í–‰ (ì¬ì‹œë„ í¬í•¨)
                function_response_parts = []
                for fc in function_calls:
                    if fc.name == "search_web":
                        query = fc.args.get("query", "")
                        result = self._search_web_with_retry(query)
                        function_response_parts.append(
                            types.Part.from_function_response(
                                name="search_web",
                                response={"result": result}
                            )
                        )

                # í•¨ìˆ˜ ê²°ê³¼ë¥¼ contentsì— ì¶”ê°€ (user role)
                contents.append(
                    types.Content(role="user", parts=function_response_parts)
                )

            else:
                # í…ìŠ¤íŠ¸ ì‘ë‹µ (ìµœì¢… ê²°ê³¼ - Markdown)
                if text_response:
                    self.log('ìµœì¢… ê²°ê³¼ ìˆ˜ì‹ ', 'result')
                    return text_response
                break

        self.log(f'ë£¨í”„ ì¢…ë£Œ (ê²€ìƒ‰ {self._search_count}íšŒ)', 'info')
        return self._build_partial_result()

    def _build_partial_result(self) -> str:
        """ì¤‘ê°„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Markdownìœ¼ë¡œ ì •ë¦¬"""
        self.log('ë¶€ë¶„ ê²°ê³¼ ìƒì„± ì¤‘...', 'info')

        intermediate = self.execution.intermediate_data or {}
        searches = intermediate.get('searches', [])

        if not searches:
            return "# ë¦¬ì„œì¹˜ ê²°ê³¼\n\nê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        result = "# ë¦¬ì„œì¹˜ ê²°ê³¼ (ë¶€ë¶„)\n\n"
        for i, search in enumerate(searches, 1):
            result += f"## {i}. {search.get('query', '')}\n\n"
            result += search.get('summary', '') + "\n\n"

        return result

    def _save_research(self, topic_title: str, result_text: str):
        """Research ëª¨ë¸ì— ë¦¬ì„œì¹˜ ê²°ê³¼ ì €ì¥ (content_analysisì—)"""
        # ì¤‘ë³µ ì œê±°ëœ ì¶œì²˜
        unique_sources = []
        seen_urls = set()

        for src in self._all_sources:
            url = src.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_sources.append({
                    'title': src.get('title', ''),
                    'url': url,
                })

        # ê¸°ì¡´ Research ê°€ì ¸ì˜¤ê¸°
        research = self.project.research
        content_analysis = research.content_analysis or {}

        # ë¦¬ì„œì¹˜ ê²°ê³¼ ì¶”ê°€
        content_analysis['research_result'] = result_text

        # sourcesë„ ì—…ë°ì´íŠ¸
        research.content_analysis = content_analysis
        research.sources = unique_sources[:20]
        research.topic = topic_title or research.topic
        research.save()

        self.log(f'ì €ì¥ ì™„ë£Œ: ì¶œì²˜ {len(unique_sources)}ê°œ', 'info')
